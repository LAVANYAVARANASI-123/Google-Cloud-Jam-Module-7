{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZoUR6VSq2qtGxRTMj4M0X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LAVANYAVARANASI-123/Google-Cloud-Jam-Module-7/blob/main/nlpexternalpractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exercise 1"
      ],
      "metadata": {
        "id": "li4hmIG7U9rS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3680R1m0KQu",
        "outputId": "8bdd0f01-d397-4977-9686-b86615761fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this @code is very #easy to    do\n",
            "  @code   very #easy to    do\n",
            "this  is very  to    do\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def noise_removal(text):\n",
        "  noise_words=['is','a','this']\n",
        "  clean_text=text\n",
        "  for word in noise_words:\n",
        "    pattern=r'\\b{}\\b'.format(word)\n",
        "    clean_text =re.sub(pattern,' ',clean_text)\n",
        "  hastagremoval=re.sub(r'#\\w+\\s|@\\w+\\s|\\s',' ',text)\n",
        "  return clean_text,hastagremoval\n",
        "text=\"this @code is very #easy to    do\"\n",
        "clean_text,hastagremoval=noise_removal(text)\n",
        "print(text)\n",
        "print(clean_text)\n",
        "print(hastagremoval)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_urls(text):\n",
        "    # Regular expression pattern to match URLs\n",
        "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
        "    # Substitute URLs with an empty string\n",
        "    clean_text = re.sub(url_pattern, '', text)\n",
        "    return clean_text\n",
        "\n",
        "# Sample text containing URLs\n",
        "sample_text = \"Check out this website: https://www.example.com and also visit http://another-example.com for more information.\"\n",
        "\n",
        "# Test the function\n",
        "cleaned_text = remove_urls(sample_text)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nCleaned Text:\")\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFpKayr6_T3_",
        "outputId": "c8e95208-a41a-42aa-b836-4052271a6880"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Check out this website: https://www.example.com and also visit http://another-example.com for more information.\n",
            "\n",
            "Cleaned Text:\n",
            "Check out this website:  and also visit  for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_specific_emoticons(text):\n",
        "    # Regular expression pattern to match specific emoticons\n",
        "    emoticon_pattern = r'[:;]-?[)D]|;-\\)'\n",
        "    # Find all specific emoticons in the text\n",
        "    emoticons = re.findall(emoticon_pattern, text)\n",
        "    return emoticons\n",
        "\n",
        "# Sample text containing specific emoticons\n",
        "sample_text = \"Hello there! :) How are you? :-D I'm feeling great ;-) but also a bit tired :-/. Let's meet up soon! ^_^\"\n",
        "\n",
        "# Test the function\n",
        "extracted_emoticons = extract_specific_emoticons(sample_text)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nExtracted Specific Emoticons:\")\n",
        "print(extracted_emoticons)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvYomaT8Ai_S",
        "outputId": "ade1f339-e5d5-49fb-9d1a-1321c1f78629"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Hello there! :) How are you? :-D I'm feeling great ;-) but also a bit tired :-/. Let's meet up soon! ^_^\n",
            "\n",
            "Extracted Specific Emoticons:\n",
            "[':)', ':-D', ';-)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    # Regular expression pattern to match all characters except alphabets and numbers\n",
        "    special_char_pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    # Substitute special characters with an empty string\n",
        "    clean_text = re.sub(special_char_pattern, '', text)\n",
        "    return clean_text\n",
        "\n",
        "# Sample text containing special characters\n",
        "sample_text = \"Hello there! How are you? This is a sample text with special characters: @, $, %, ^, &, *, (, ), -, +, =.\"\n",
        "\n",
        "# Test the function\n",
        "cleaned_text = remove_special_characters(sample_text)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nCleaned Text:\")\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHHAxMoaCuD7",
        "outputId": "c58c0d31-a468-40ec-a2e2-2451425f1a7a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Hello there! How are you? This is a sample text with special characters: @, $, %, ^, &, *, (, ), -, +, =.\n",
            "\n",
            "Cleaned Text:\n",
            "Hello there How are you This is a sample text with special characters           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exercise 2"
      ],
      "metadata": {
        "id": "NfCsLlxtU4Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "porter_stemmer = PorterStemmer()\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "def toke_stem(text):\n",
        "  tokenizer= word_tokenize(text)\n",
        "  stemmer=[porter_stemmer.stem(word) for word in tokenizer]\n",
        "  return ' '.join(stemmer)\n",
        "def toke_lemm(text):\n",
        "  tokenizer= word_tokenize(text)\n",
        "  lemmatized=[lemmatizer.lemmatize(word) for word in tokenizer]\n",
        "  return ' '.join(lemmatized)\n",
        "def pos_tagger(text):\n",
        "  tokenizer=word_tokenize(text)\n",
        "  pos_tagged=pos_tag(tokenizer)\n",
        "  return pos_tagged\n",
        "text='the quick brown foxes jumped over lazy a dog'\n",
        "print(text)\n",
        "print(toke_stem(text))\n",
        "print(toke_lemm(text))\n",
        "print(pos_tagger(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_5frWFsgsEo",
        "outputId": "73cdf90c-f288-4197-a331-db50e23b159e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the quick brown foxes jumped over lazy a dog\n",
            "the quick brown fox jump over lazi a dog\n",
            "the quick brown fox jumped over lazy a dog\n",
            "[('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('foxes', 'NNS'), ('jumped', 'VBD'), ('over', 'IN'), ('lazy', 'FW'), ('a', 'DT'), ('dog', 'NN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize the stemmers\n",
        "porter_stemmer = PorterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def tokenize_and_stem(text, stemmer):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "    # Stem the tokens\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown foxes jumped over the lazy dogs. They were running quickly to catch the frisbee.\"\n",
        "\n",
        "# Stemming with Porter Stemmer\n",
        "porter_stemmed_text = tokenize_and_stem(text, porter_stemmer)\n",
        "\n",
        "# Stemming with Snowball Stemmer\n",
        "snowball_stemmed_text = tokenize_and_stem(text, snowball_stemmer)\n",
        "\n",
        "# Printing the results\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nPorter Stemmed Text:\")\n",
        "print(porter_stemmed_text)\n",
        "print(\"\\nSnowball Stemmed Text:\")\n",
        "print(snowball_stemmed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyH-7DYNnbEC",
        "outputId": "995efb04-31f3-47a7-a05a-0e25e965e944"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "The quick brown foxes jumped over the lazy dogs. They were running quickly to catch the frisbee.\n",
            "\n",
            "Porter Stemmed Text:\n",
            "quick brown fox jump lazi dog . run quickli catch frisbe .\n",
            "\n",
            "Snowball Stemmed Text:\n",
            "quick brown fox jump lazi dog . run quick catch frisbe .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Custom dictionary for medical terms\n",
        "medical_terms = {\n",
        "    'diagnoses': 'diagnosis',\n",
        "    'diagnosing': 'diagnosis',\n",
        "    'diagnosed': 'diagnosis',\n",
        "    'symptoms': 'symptom',\n",
        "    'symptomatic': 'symptom',\n",
        "    'analyses': 'analysis',\n",
        "    'analysing': 'analysis',\n",
        "    'analyzed': 'analysis',\n",
        "    'biopsies': 'biopsy',\n",
        "    'biopsying': 'biopsy',\n",
        "    'metastases': 'metastasis',\n",
        "    'metastasized': 'metastasis',\n",
        "}\n",
        "\n",
        "def custom_lemmatize(token):\n",
        "    # Check if the token is in the custom dictionary\n",
        "    if token.lower() in medical_terms:\n",
        "        return medical_terms[token.lower()]\n",
        "    else:\n",
        "        # Use the standard lemmatizer\n",
        "        return lemmatizer.lemmatize(token)\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [custom_lemmatize(token) for token in tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Sample medical text\n",
        "text = \"The patient was diagnosed with several metastases and underwent multiple biopsies. The symptoms were symptomatic of a severe diagnosis.\"\n",
        "\n",
        "# Apply the custom lemmatization\n",
        "lemmatized_text = lemmatize_text(text)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nLemmatized Text:\")\n",
        "print(lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0Y1Zgol-m5s",
        "outputId": "d93104a5-8f09-4022-e1ec-67285d715d2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "The patient was diagnosed with several metastases and underwent multiple biopsies. The symptoms were symptomatic of a severe diagnosis.\n",
            "\n",
            "Lemmatized Text:\n",
            "The patient wa diagnosis with several metastasis and underwent multiple biopsy . The symptom were symptom of a severe diagnosis .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exercise-3"
      ],
      "metadata": {
        "id": "h3tu6FUPBZrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slangs_map={\n",
        "    'lol':'laugh a lot',\n",
        "    'omg':'oh my god',\n",
        "    'tq':'thank you',\n",
        "    'atb':'all the best',\n",
        "    'hlo':'hello'\n",
        "}\n",
        "def standard_text(text,slangs_map):\n",
        "  for slangs,standard in slangs_map.items():\n",
        "    text=text.replace(slangs,standard)\n",
        "  return text\n",
        "text=\"hlo tq for suggesting the atb movie omg it was too lol\"\n",
        "print(text)\n",
        "print(standard_text(text,slangs_map))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-0L1GQnBcML",
        "outputId": "f668b407-ceca-4916-a56d-91a0e44fc418"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hlo tq for suggesting the atb movie omg it was too lol\n",
            "hello thank you for suggesting the all the best movie oh my god it was too laugh a lot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Define a dictionary for social media slangs and their standard forms\n",
        "slangs_map = {\n",
        "    \"lol\": \"laugh out loud\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"hlo\": \"hello\",\n",
        "    \"tq\": \"thank you\"\n",
        "}\n",
        "\n",
        "# Function to standardize text and provide slang replacement statistics\n",
        "def standard_text(text, slangs_map):\n",
        "    slang_counter = Counter()\n",
        "    for slang, standard in slangs_map.items():\n",
        "        occurrences = text.count(slang)\n",
        "        if occurrences > 0:\n",
        "            slang_counter[slang] += occurrences\n",
        "            text = text.replace(slang, standard)\n",
        "    return text, slang_counter\n",
        "\n",
        "# Sample text containing social media slangs\n",
        "text = \"hlo the movie omg that you suggested was lol tq for it. hlo again!\"\n",
        "\n",
        "# Standardize the text and get the slang replacement statistics\n",
        "standardized_text, slang_counter = standard_text(text, slangs_map)\n",
        "\n",
        "# Print the results\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nStandardized Text:\")\n",
        "print(standardized_text)\n",
        "print(\"\\nSlang Replacement Statistics:\")\n",
        "print(\"Total slangs replaced:\", sum(slang_counter.values()))\n",
        "print(\"Most common slangs and their frequencies:\", slang_counter.most_common())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYjVLMvUAa5Q",
        "outputId": "e9060d62-9a7c-471a-bfed-2ee010670468"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "hlo the movie omg that you suggested was lol tq for it. hlo again!\n",
            "\n",
            "Standardized Text:\n",
            "hello the movie oh my god that you suggested was laugh out loud thank you for it. hello again!\n",
            "\n",
            "Slang Replacement Statistics:\n",
            "Total slangs replaced: 5\n",
            "Most common slangs and their frequencies: [('hlo', 2), ('lol', 1), ('omg', 1), ('tq', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Define a dictionary for social media slangs and their standard forms\n",
        "slangs_map = {\n",
        "    \"lol\": \"laugh out loud\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"hlo\": \"hello\",\n",
        "    \"tq\": \"thank you\"\n",
        "}\n",
        "\n",
        "# Function to standardize text and provide slang replacement statistics\n",
        "def standard_text(text, slangs_map):\n",
        "    slang_counter = Counter()\n",
        "    for slang, standard in slangs_map.items():\n",
        "        # Use regex to find whole word matches only\n",
        "        pattern = r'\\b{}\\b'.format(re.escape(slang))\n",
        "        occurrences = len(re.findall(pattern, text))\n",
        "        if occurrences > 0:\n",
        "            slang_counter[slang] += occurrences\n",
        "            text = re.sub(pattern, standard, text)\n",
        "    return text, slang_counter\n",
        "\n",
        "# Sample text containing social media slangs\n",
        "text = \"hlo the movie omg that you suggested was lol tq for it. hlo again!\"\n",
        "\n",
        "# Standardize the text and get the slang replacement statistics\n",
        "standardized_text, slang_counter = standard_text(text, slangs_map)\n",
        "\n",
        "# Print the results\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nStandardized Text:\")\n",
        "print(standardized_text)\n",
        "print(\"\\nSlang Replacement Statistics:\")\n",
        "print(\"Total slangs replaced:\", sum(slang_counter.values()))\n",
        "print(\"Most common slangs and their frequencies:\", slang_counter.most_common())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01k0jAHRjTQK",
        "outputId": "c1bda3da-fa63-419b-9791-5652072443db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "hlo the movie omg that you suggested was lol tq for it. hlo again!\n",
            "\n",
            "Standardized Text:\n",
            "hello the movie oh my god that you suggested was laugh out loud thank you for it. hello again!\n",
            "\n",
            "Slang Replacement Statistics:\n",
            "Total slangs replaced: 5\n",
            "Most common slangs and their frequencies: [('hlo', 2), ('lol', 1), ('omg', 1), ('tq', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define a dictionary for social media slangs and their standard forms\n",
        "slangs_map = {\n",
        "    \"lol\": \"laugh out loud\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"hlo\": \"hello\",\n",
        "    \"tq\": \"thank you\"\n",
        "}\n",
        "\n",
        "# Define a dictionary for emojis and their textual representations\n",
        "emoji_map = {\n",
        "    \"ğŸ˜€\": \"smile\",\n",
        "    \"ğŸ˜‚\": \"laugh\",\n",
        "    \"ğŸ˜‰\": \"wink\",\n",
        "    \"ğŸ˜\": \"love\",\n",
        "    \"ğŸ˜­\": \"cry\",\n",
        "    \"ğŸ˜¡\": \"angry\",\n",
        "    \"ğŸ‘\": \"thumbs up\",\n",
        "    \"ğŸ™\": \"pray\"\n",
        "}\n",
        "\n",
        "# Function to standardize text by replacing slangs and emojis\n",
        "def standardize_text(text, slangs_map, emoji_map):\n",
        "    # Replace slangs\n",
        "    for slang, standard in slangs_map.items():\n",
        "      text=text.replace(slang,standard)\n",
        "    return text\n",
        "    # Replace emojis\n",
        "    for emoji, description in emoji_map.items():\n",
        "        text = text.replace(emoji, description)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Sample text containing social media slangs and emojis\n",
        "text = \"hlo the movie omg that you suggested was lol tq for it ğŸ˜‚. hlo again! ğŸ˜€\"\n",
        "\n",
        "# Standardize the text\n",
        "standardized_text = standardize_text(text, slangs_map, emoji_map)\n",
        "\n",
        "# Print the results\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nStandardized Text:\")\n",
        "print(standardized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g_K4VjJpEpp",
        "outputId": "99023a81-eca0-4953-a257-494accb3af13"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "hlo the movie omg that you suggested was lol tq for it ğŸ˜‚. hlo again! ğŸ˜€\n",
            "\n",
            "Standardized Text:\n",
            "hello the movie oh my god that you suggested was laugh out loud thank you for it ğŸ˜‚. hello again! ğŸ˜€\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exercise 4"
      ],
      "metadata": {
        "id": "_ZStQFkKLMmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def pos_tagger(text):\n",
        "  tokens=word_tokenize(text)\n",
        "  pos_tagged=pos_tag(tokens)\n",
        "  return pos_tagged\n",
        "text='the quick brown foxes jumped over lazy a dog'\n",
        "pos_tagg=pos_tagger(text)\n",
        "print(text)\n",
        "print(pos_tagg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUKH4L7YLOJG",
        "outputId": "c3d954c5-faf8-43dc-d918-530fcd02d1ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the quick brown foxes jumped over lazy a dog\n",
            "[('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('foxes', 'NNS'), ('jumped', 'VBD'), ('over', 'IN'), ('lazy', 'FW'), ('a', 'DT'), ('dog', 'NN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def pos_tagging_and_ner(text):\n",
        "    # Tokenize the text into words\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Perform POS tagging\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Perform Named Entity Recognition\n",
        "    named_entities = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "    return pos_tags, named_entities\n",
        "\n",
        "# Sample text\n",
        "text = \"Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.\"\n",
        "\n",
        "# Perform POS tagging and NER\n",
        "pos_tags, named_entities = pos_tagging_and_ner(text)\n",
        "\n",
        "# Print POS tags\n",
        "print(\"Part of Speech Tags:\")\n",
        "print(text)\n",
        "print(pos_tags)\n",
        "\n",
        "#or for token, pos in pos_tags:\n",
        "   # print(f\"{token}: {pos}\")\n",
        "\n",
        "# Print Named Entities\n",
        "print(\"\\nNamed Entities:\")\n",
        "print(named_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoIU2PhLNbhD",
        "outputId": "6836052f-36e2-40b2-9baa-3b06999aaa85"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part of Speech Tags:\n",
            "Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.\n",
            "[('Barack', 'NNP'), ('Obama', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('on', 'IN'), ('August', 'NNP'), ('4', 'CD'), (',', ','), ('1961', 'CD'), (',', ','), ('in', 'IN'), ('Honolulu', 'NNP'), (',', ','), ('Hawaii', 'NNP'), ('.', '.')]\n",
            "\n",
            "Named Entities:\n",
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  born/VBN\n",
            "  on/IN\n",
            "  August/NNP\n",
            "  4/CD\n",
            "  ,/,\n",
            "  1961/CD\n",
            "  ,/,\n",
            "  in/IN\n",
            "  (GPE Honolulu/NNP)\n",
            "  ,/,\n",
            "  (GPE Hawaii/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from textblob import TextBlob\n",
        "!pip install stanza\n",
        "!pip install flair\n",
        "import stanza\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from polyglot.text import Text\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download necessary NLTK data and models for Stanza\n",
        "stanza.download('en')\n",
        "\n",
        "# Initialize NLP models\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "stanza_nlp = stanza.Pipeline('en')\n",
        "flair_tagger = SequenceTagger.load('pos')\n",
        "transformers_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "def spacy_pos_tagging(text):\n",
        "    doc = spacy_nlp(text)\n",
        "    return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "def textblob_pos_tagging(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.tags\n",
        "\n",
        "def stanza_pos_tagging(text):\n",
        "    doc = stanza_nlp(text)\n",
        "    pos_tags = [(word.text, word.upos) for sentence in doc.sentences for word in sentence.words]\n",
        "    return pos_tags\n",
        "\n",
        "def flair_pos_tagging(text):\n",
        "    sentence = Sentence(text)\n",
        "    flair_tagger.predict(sentence)\n",
        "    return [(entity.text, entity.get_tag('pos').value) for entity in sentence]\n",
        "\n",
        "def polyglot_pos_tagging(text):\n",
        "    polyglot_text = Text(text)\n",
        "    return [(word, word.tag) for word in polyglot_text.pos_tags]\n",
        "\n",
        "def transformers_pos_tagging(text):\n",
        "    ner_results = transformers_pipeline(text)\n",
        "    return [(result['word'], result['entity']) for result in ner_results]\n",
        "\n",
        "# Sample text\n",
        "text = \"Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.\"\n",
        "\n",
        "# Perform POS tagging using different libraries\n",
        "spacy_tags = spacy_pos_tagging(text)\n",
        "textblob_tags = textblob_pos_tagging(text)\n",
        "stanza_tags = stanza_pos_tagging(text)\n",
        "flair_tags = flair_pos_tagging(text)\n",
        "polyglot_tags = polyglot_pos_tagging(text)\n",
        "transformers_tags = transformers_pos_tagging(text)\n",
        "\n",
        "# Print results\n",
        "print(\"SpaCy POS Tags:\")\n",
        "print(spacy_tags)\n",
        "\n",
        "print(\"\\nTextBlob POS Tags:\")\n",
        "print(textblob_tags)\n",
        "\n",
        "print(\"\\nStanfordNLP (Stanza) POS Tags:\")\n",
        "print(stanza_tags)\n",
        "\n",
        "print(\"\\nFlair POS Tags:\")\n",
        "print(flair_tags)\n",
        "\n",
        "print(\"\\nPolyglot POS Tags:\")\n",
        "print(polyglot_tags)\n",
        "\n",
        "print(\"\\nTransformers POS Tags:\")\n",
        "print(transformers_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D4GqUdAzbFxi",
        "outputId": "6aa75e0b-7ba1-4e32-b4b6-8e846319859d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.10/dist-packages (1.8.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza) (2.12.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->stanza) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
            "Collecting flair\n",
            "  Downloading flair-0.13.1-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3>=1.20.27 (from flair)\n",
            "  Downloading boto3-1.34.116-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.2 (from flair)\n",
            "  Downloading bpemb-0.3.5-py3-none-any.whl (19 kB)\n",
            "Collecting conllu>=4.0 (from flair)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Collecting deprecated>=1.2.13 (from flair)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting ftfy>=6.1.0 (from flair)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair) (5.1.0)\n",
            "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from flair) (0.23.1)\n",
            "Collecting janome>=0.4.2 (from flair)\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect>=1.0.9 (from flair)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.9.4)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (3.7.1)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.10/dist-packages (from flair) (10.1.0)\n",
            "Collecting mpld3>=0.3 (from flair)\n",
            "  Downloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pptree>=3.1 (from flair)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from flair) (2.8.2)\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from flair) (2024.5.15)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.2)\n",
            "Collecting segtok>=1.5.11 (from flair)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.66.4)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
            "  Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: transformers[sentencepiece]<5.0.0,>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.41.1)\n",
            "Collecting urllib3<2.0.0,>=1.0.0 (from flair)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia-api>=0.5.7 (from flair)\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Collecting semver<4.0.0,>=3.0.0 (from flair)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Collecting botocore<1.35.0,>=1.34.116 (from boto3>=1.20.27->flair)\n",
            "  Downloading botocore-1.34.116-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (0.1.99)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (3.14.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (6.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (4.11.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect>=1.0.9->flair) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (3.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mpld3>=0.3->flair) (3.1.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (3.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.8,>=1.5.0->flair) (12.5.40)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.4.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (3.20.3)\n",
            "Collecting accelerate>=0.21.0 (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair)\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mpld3>=0.3->flair) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (5.9.5)\n",
            "Building wheels for collected packages: langdetect, pptree, sqlitedict\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=70ec0aa57ff20e2578f263580b5ba8dccf44e654bfe6c7a41916e48860433d43\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=2c99447959f73ace414b3ae107ddd4716a2300cd345c102089805b8e02ed1718\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=3c19fff64ab945932d5eac2171c33c50c57a6b943f9337da0d1583f18ddc84ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built langdetect pptree sqlitedict\n",
            "Installing collected packages: sqlitedict, pptree, janome, urllib3, semver, segtok, langdetect, jmespath, ftfy, deprecated, conllu, botocore, wikipedia-api, s3transfer, mpld3, bpemb, pytorch-revgrad, boto3, accelerate, transformer-smaller-training-vocab, flair\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "Successfully installed accelerate-0.30.1 boto3-1.34.116 botocore-1.34.116 bpemb-0.3.5 conllu-4.5.3 deprecated-1.2.14 flair-0.13.1 ftfy-6.2.0 janome-0.5.0 jmespath-1.0.1 langdetect-1.0.9 mpld3-0.5.10 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.10.1 segtok-1.5.11 semver-3.0.2 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.4.0 urllib3-1.26.18 wikipedia-api-0.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              },
              "id": "c22fb43ec62d4c48bc5f6ce3f11567e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'urllib3.util' has no attribute 'PROTOCOL_TLS'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-de06dcae6ce2>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install flair'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flair/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# global variable: cache_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_proxies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcache_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FLAIR_CACHE_ROOT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".flair\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flair/file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/boto3/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_warn_deprecated_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0m__author__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Amazon Web Services'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/boto3/session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownServiceError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxform_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientArgsCreator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUTH_TYPE_MAPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/waiter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjmespath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWaiterDocstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_service_module_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/docs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mServiceDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mDEPRECATED_SERVICE_NAMES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'sms-voice'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/docs/service.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# language governing permissions and limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbcdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocumentStructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from botocore.docs.client import (\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mClientContextParamsDocumenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mClientDocumenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/docs/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbcdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocumentStructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResponseExampleDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m from botocore.docs.method import (\n\u001b[1;32m     20\u001b[0m     \u001b[0mdocument_custom_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/docs/example.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# ANY KIND, either express or implied. See the License for the specific\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# language governing permissions and limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShapeDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/docs/shape.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# inherited from a Documenter class with the appropriate methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# and attributes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_json_value_header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawsrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttpsession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# IP Regexes retained for backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Always import the original SSLContext, even if it has been patched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         from urllib3.contrib.pyopenssl import (\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0morig_util_SSLContext\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSSLContext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Map from urllib3 to PyOpenSSL compatible parameter-values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m _openssl_versions = {\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROTOCOL_TLS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLv23_METHOD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mPROTOCOL_TLS_CLIENT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLv23_METHOD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROTOCOL_TLSv1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTLSv1_METHOD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'urllib3.util' has no attribute 'PROTOCOL_TLS'"
          ]
        }
      ]
    }
  ]
}