{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyRZpLXyO9XOAeD3G1xtZh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LAVANYAVARANASI-123/Google-Cloud-Jam-Module-7/blob/main/till6a%2Brev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsNOL47_CsdL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "#re - regular expression which helps for pattern matching"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''if you want to match any digits like a phone number we can check our re by going to \"regex101.com\" and using respective symbol displayed at explanation\n",
        "for matching a single digit we use /d\n",
        "for 2 digits /d/d and so on ..\n",
        "but inorder to simply match all the 10 digits at a time we use /d{10} = excatly 10 of d(0-9)'''"
      ],
      "metadata": {
        "id": "8NShW27jYAgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EKfZdzX4X5iI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat1='hello welcome! whats up ,to contact me msg to 9182017247 or mailto lavanyavaranasi3@gmail.com'\n",
        "chat2=\"hello welcome! whats up ,to contact me msg to 918-201-7247 or mailto lavanyavaranasi3@gmail.com\""
      ],
      "metadata": {
        "id": "BD2ihFkDFD8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matchcheker=re.findall('\\d{10}',chat1)\n",
        "matchcheker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyKCXTSYGazY",
        "outputId": "6f47038c-d6bf-443e-faac-792c3bc33e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['9182017247']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "''' in order to match 091-233-4554 this kind of number we use \" \\(d{3}\\)-\\d{3}-\\d{4} '''"
      ],
      "metadata": {
        "id": "SnbyHYIaYIvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "match2=re.findall('\\d{10}|\\d{3}\\-\\d{3}-\\d{4}',chat2)\n",
        "match2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJR4CgcEKN0J",
        "outputId": "052055de-f4e2-41e6-f60e-f8af69c27bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['918-201-7247']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "f8FpBKCpTPrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** using spacy (oops)**"
      ],
      "metadata": {
        "id": "jApihrlrRfTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\"Dr. Lavanya ! this is me , and you will. whats up?\")\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUTO3rxzR1ev",
        "outputId": "821d6ee3-33d9-4faf-8650-eb6f082f8231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr. Lavanya !\n",
            "this is me , and you will.\n",
            "whats up?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in doc.sents:\n",
        "  for words in sentence:\n",
        "    print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bm3FTZxTIjc",
        "outputId": "8dfc684b-1724-40b0-e47f-0fca6d0f9081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr.\n",
            "lavanya\n",
            "!\n",
            "this\n",
            "is\n",
            "me\n",
            ",\n",
            "and\n",
            "you\n",
            "will\n",
            ".\n",
            "what\n",
            "s\n",
            "up\n",
            "?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*NLTK (string processing)*"
      ],
      "metadata": {
        "id": "OfK7yGXXUElg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJScnd0RUNfL",
        "outputId": "3dbbed33-b5fc-42b9-af1a-5242a0476649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SENTENCE TOKENIZATION**"
      ],
      "metadata": {
        "id": "Qt5InLwrYYZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize( \"Dr. Lavanya ! this is me , and you will. whats up?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjIADPNLUT7G",
        "outputId": "9c888cc1-3435-4324-c4c6-19a3f15c7fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr. Lavanya !', 'this is me , and you will.', 'whats up?']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEED TO VERIFY THIS *In spacy if you choose sentence tokenizer and gave input as Dr. then it considers as word itself but if you consider NLTK it takes it as another sentence *"
      ],
      "metadata": {
        "id": "2Cxej83wXCxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORD TOKENIZER**"
      ],
      "metadata": {
        "id": "cQyXQsLyYk9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(\"Dr. lavanya ! this is me , and you will. whats up?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-KEeO4UXt0f",
        "outputId": "9b2c84d3-f495-4e9d-bb21-0e06c5e27189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello',\n",
              " 'dr.',\n",
              " 'lavanya',\n",
              " '!',\n",
              " 'this',\n",
              " 'is',\n",
              " 'me',\n",
              " ',',\n",
              " 'and',\n",
              " 'you',\n",
              " 'will',\n",
              " '.',\n",
              " 'whats',\n",
              " 'up',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. *: Matches zero or more occurrences.\n",
        "2. +: Matches one or more occurrences.\n",
        "3.  ?: Matches zero or one occurrence.\n",
        "4.  {n}: Matches exactly n occurrences.\n",
        "5.  {n,}: Matches n or more occurrences.\n",
        "6.  {n,m}: Matches at least n and at most m occurrences.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l-z4ugtMBMH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_noise(text):\n",
        "    # Remove noise containing common words\n",
        "    noise_words = ['is', 'a', 'this']\n",
        "    cleaned_text_words = text\n",
        "    for word in noise_words:\n",
        "        pattern = r'\\b{}\\b'.format(word)\n",
        "        cleaned_text_words = re.sub(pattern, '', cleaned_text_words)\n",
        "\n",
        "    # Remove words following hashtags\n",
        "    cleaned_text_hashtags = re.sub(r'#\\w+\\s*', '', text)\n",
        "\n",
        "    # Remove \"@\" symbols\n",
        "    cleaned_text_hashtags = re.sub(r'@\\w+', '', cleaned_text_hashtags)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    cleaned_text_hashtags = re.sub(r'\\s+', ' ', cleaned_text_hashtags)\n",
        "\n",
        "    return cleaned_text_words, cleaned_text_hashtags\n",
        "\n",
        "# Example text with noise containing common words and hashtags\n",
        "text = \"This is a #sample text with @some noise!   12345 #removeme #notthisone\"\n",
        "\n",
        "# Remove noise containing common words and hashtags separately\n",
        "cleaned_text_words, cleaned_text_hashtags = remove_noise(text)\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nCleaned Text (Removing noise containing common words):\")\n",
        "print(cleaned_text_words)\n",
        "print(\"\\nCleaned Text (Removing words following hashtags):\")\n",
        "print(cleaned_text_hashtags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsL-1iRLNv2S",
        "outputId": "1aa562bb-75c8-477a-dcfc-60d096657f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "This is a #sample text with @some noise!   12345 #removeme #notthisone\n",
            "\n",
            "Cleaned Text (Removing noise containing common words):\n",
            "This   #sample text with @some noise!   12345 #removeme #notthisone\n",
            "\n",
            "Cleaned Text (Removing words following hashtags):\n",
            "This is a text with noise! 12345 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def noise_removal(text):\n",
        "  noise_words=['is','a','this']\n",
        "  clean_text=text\n",
        "  for word in noise_words:\n",
        "    position=r'\\b{}\\b'.format(word)\n",
        "    clean_text=re.sub(position,'',clean_text)\n",
        "  hashtag_removal=re.sub(r'#\\w+\\s|@\\w+\\s','',text)\n",
        "  return clean_text,hashtag_removal\n",
        "text=\"this code is very #easy @to  do  \"\n",
        "clean_text,hashtag_removal=noise_removal(text)\n",
        "print(text)\n",
        "print(clean_text)\n",
        "print(hashtag_removal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jODJ109pYEfu",
        "outputId": "d80fc0a6-8dd3-4015-dd15-9ba1bede2233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this code is very #easy @to  do  \n",
            " code  very #easy @to  do  \n",
            "this code is very  do  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def noise_removal(text):\n",
        "  for word in text:\n",
        "        pattern = r'\\b{}\\b'.format(word)\n",
        "  hashremoval=re.sub(r'@\\w+|#\\w+\\s|\\s',' ',text)\n",
        "  return hashremoval\n",
        "text=\"this code is very #easy is @it \"\n",
        "print(noise_removal(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBJYrBHCcuj9",
        "outputId": "b72a268e-90ad-4f93-c6cf-6f9b5123b6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this code is very  is   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**media slangs to standard text**"
      ],
      "metadata": {
        "id": "7Uu13Q-GUD6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slangs_map={\n",
        "    \"lol\":\"laugh out loud\",\n",
        "    \"omg\":\"oh my god\",\n",
        "    \"hlo\":\"hello\",\n",
        "    \"tq\":\"thank you\"\n",
        "}\n",
        "def standard_text(text,slangs_map):\n",
        "  for slang,standard in slangs_map.items():\n",
        "    text=text.replace(slang,standard)\n",
        "  return text\n",
        "text=\"hlo the movie omg that you suggested was lol tq for it\"\n",
        "standardized_text=standard_text(text,slangs_map)\n",
        "print(text)\n",
        "print(standardized_text)"
      ],
      "metadata": {
        "id": "AEycaun3Sp6-",
        "outputId": "505babc2-9bea-4d0a-b307-8a11061273c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hlo the movie omg that you suggested was lol tq for it\n",
            "hello the movie oh my god that you suggested was laugh out loud thank you for it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS TAGGER**"
      ],
      "metadata": {
        "id": "DqxtSwePoV6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text=\"hi my name is lavanya , i love to improve myself\"\n",
        "tokens=word_tokenize(text)\n",
        "print(tokens)\n",
        "pos_tagger=pos_tag(tokens)\n",
        "print(pos_tagger)\n"
      ],
      "metadata": {
        "id": "4GL9wpUpUMkY",
        "outputId": "112a094b-59af-42ac-9da0-22d11d26146b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi', 'my', 'name', 'is', 'lavanya', ',', 'i', 'love', 'to', 'improve', 'myself']\n",
            "[('hi', 'NN'), ('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('lavanya', 'JJ'), (',', ','), ('i', 'JJ'), ('love', 'VBP'), ('to', 'TO'), ('improve', 'VB'), ('myself', 'PRP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def pos_tagger(text):\n",
        "  tokens=word_tokenize(text)\n",
        "  pos_framer=pos_tag(tokens)\n",
        "  return pos_framer\n",
        "text=\"hi my name is lavanya , i love to improve myself\"\n",
        "pos_converter=pos_tagger(text)\n",
        "print(text)\n",
        "print(pos_converter)"
      ],
      "metadata": {
        "id": "-1yW1NQeWDHp",
        "outputId": "870039a9-0e23-430f-e7e0-ee1771b1a884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi my name is lavanya , i love to improve myself\n",
            "[('hi', 'NN'), ('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('lavanya', 'JJ'), (',', ','), ('i', 'JJ'), ('love', 'VBP'), ('to', 'TO'), ('improve', 'VB'), ('myself', 'PRP')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**"
      ],
      "metadata": {
        "id": "RkzAA_4Foav1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "corpus=[\"data science is one of the most important fields of science\",\n",
        "    \"this is one of the best data science courses\",\n",
        "    \"data scientists analyze data\"]\n",
        "words_set=set()\n",
        "for doc in corpus:\n",
        "  words=doc.split(' ')\n",
        "  words_set=words_set.union(set(words))\n",
        "n_docs=len(corpus)\n",
        "n_words_set=len(words_set)\n",
        "df_tf=pd.DataFrame(np.zeros((n_docs,n_words_set)),columns=list(words_set))\n",
        "#tf\n",
        "for i in range(n_docs):\n",
        "  words=corpus[i].split(' ')\n",
        "  for w in words:\n",
        "    df_tf[w][i]=df_tf[w][i]+(1/len(words))\n",
        "#idf\n",
        "idf={}\n",
        "for w in words_set:\n",
        "  k=0\n",
        "  for i in range(n_docs):\n",
        "    if w in corpus[i].split(' '):\n",
        "      k+=1\n",
        "  idf[w]=np.log10(n_docs/k)\n",
        "#df_tf_idf\n",
        "df_tf_idf=df_tf.copy()\n",
        "for w in words_set:\n",
        "  for i in range(n_docs):\n",
        "    df_tf_idf[w][i]=df_tf[w][i]*idf[w]\n",
        "print(df_tf_idf.sort_index(axis=1))\n"
      ],
      "metadata": {
        "id": "nydIR9BjdWM6",
        "outputId": "122debf7-194e-4cc7-892d-1af78cd31047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   analyze      best   courses  data    fields  important        is      most  \\\n",
            "0  0.00000  0.000000  0.000000   0.0  0.043375   0.043375  0.016008  0.043375   \n",
            "1  0.00000  0.053013  0.053013   0.0  0.000000   0.000000  0.019566  0.000000   \n",
            "2  0.11928  0.000000  0.000000   0.0  0.000000   0.000000  0.000000  0.000000   \n",
            "\n",
            "         of       one   science  scientists       the      this  \n",
            "0  0.032017  0.016008  0.032017     0.00000  0.016008  0.000000  \n",
            "1  0.019566  0.019566  0.019566     0.00000  0.019566  0.053013  \n",
            "2  0.000000  0.000000  0.000000     0.11928  0.000000  0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "import re\n",
        "def noise_removal(text):\n",
        "  noise_words=['is','a','this']\n",
        "  clean_text=text\n",
        "  for word in noise_words:\n",
        "    pattern=r'\\b{}\\b'.format(word)\n",
        "    clean_text =re.sub(pattern,' ',clean_text)\n",
        "  hashtag_removal=re.sub(r'#\\w+\\s|@\\w+|\\s+',' ',text)\n",
        "  return clean_text,hashtag_removal\n",
        "text=\"this @code is very #easy to    do\"\n",
        "clean_text,hashtag_removal=noise_removal(text)\n",
        "print(text)\n",
        "print(clean_text)\n",
        "print(hashtag_removal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I--qToASofUm",
        "outputId": "f7baaef8-686d-48e8-d004-d11b567a8f2e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this @code is very #easy to    do\n",
            "  @code   very #easy to    do\n",
            "this   is very  to do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "porter_stemmer=PorterStemmer()\n",
        "def toke_and_stem(text):\n",
        "  tokens=word_tokenize(text)\n",
        "  token_stem_pairs=[]\n",
        "  for token in tokens:\n",
        "    stemmed_token=porter_stemmer.stem(token)\n",
        "    token_stem_pairs.append((token,stemmed_token))\n",
        "  return token_stem_pairs\n",
        "text=\"hi hello what are you doing\"\n",
        "token_stem_pairs=toke_and_stem(text)\n",
        "for token,stemmed_token in token_stem_pairs:\n",
        "  print(f\"token : {token} \\t token_stem: {stemmed_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlYjVCnGtib4",
        "outputId": "0b62ec7c-a75d-44c1-c2e8-2e6a1ffee1e5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token : hi \t token_stem: hi\n",
            "token : hello \t token_stem: hello\n",
            "token : what \t token_stem: what\n",
            "token : are \t token_stem: are\n",
            "token : you \t token_stem: you\n",
            "token : doing \t token_stem: do\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(text)\n",
        "    # Initialize a list to store token-stem pairs\n",
        "    token_stem_pairs = []\n",
        "    # Apply stemming to each token\n",
        "    for token in tokens:\n",
        "        # Stem the token\n",
        "        stemmed_token = porter_stemmer.stem(token)\n",
        "        # Append token-stem pair to the list\n",
        "        token_stem_pairs.append((token, stemmed_token))\n",
        "    return token_stem_pairs\n",
        "\n",
        "# Example text\n",
        "example_text = \"The quick brown foxes are jumping over the lazy dogs\"\n",
        "\n",
        "# Tokenize and stem the example text\n",
        "token_stem_pairs = tokenize_and_stem(example_text)\n",
        "\n",
        "# Print tokenization and stemming results\n",
        "print(\"Tokenization and Stemming:\")\n",
        "for token, stemmed_token in token_stem_pairs:\n",
        "    print(f\"Token: {token} \\t Stem: {stemmed_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuFOKfvYyAsM",
        "outputId": "c371ac69-6955-4381-818e-172e48f73231"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization and Stemming:\n",
            "Token: The \t Stem: the\n",
            "Token: quick \t Stem: quick\n",
            "Token: brown \t Stem: brown\n",
            "Token: foxes \t Stem: fox\n",
            "Token: are \t Stem: are\n",
            "Token: jumping \t Stem: jump\n",
            "Token: over \t Stem: over\n",
            "Token: the \t Stem: the\n",
            "Token: lazy \t Stem: lazi\n",
            "Token: dogs \t Stem: dog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "porter_stemmer = PorterStemmer()\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "def toke_and_stem(text):\n",
        "  tokens=word_tokenize(text)\n",
        "  stemmed_token=[porter_stemmer.stem(token) for token in tokens]\n",
        "  return ' '.join( stemmed_token)\n",
        "def toke_and_lemm(text):\n",
        "  tokens=word_tokenize(text)\n",
        "  lemmatized=[lemmatizer.lemmatize(token) for token in tokens]\n",
        "  return ' '.join( lemmatized)\n",
        "#tok=word_tokenize(text)\n",
        "#pos_tags=pos_tag(tok)\n",
        "text='the quick brown foxes jumped over lazy a dog'\n",
        "stemmer=toke_and_stem(text)\n",
        "lemma=toke_and_lemm(text)\n",
        "print(text)\n",
        "print(stemmer)\n",
        "print(lemma)\n",
        "#print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-MRgTiTy3vb",
        "outputId": "91c9e96a-7c56-4d1a-c6ab-e914aba44989"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the quick brown foxes jumped over lazy a dog\n",
            "the quick brown fox jump over lazi a dog\n",
            "the quick brown fox jumped over lazy a dog\n",
            "[('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('foxes', 'NNS'), ('jumped', 'VBD'), ('over', 'IN'), ('lazy', 'FW'), ('a', 'DT'), ('dog', 'NN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    }
  ]
}