{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMalT775y7+tsVujIYdmMev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LAVANYAVARANASI-123/Google-Cloud-Jam-Module-7/blob/main/till6a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsNOL47_CsdL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "#re - regular expression which helps for pattern matching"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''if you want to match any digits like a phone number we can check our re by going to \"regex101.com\" and using respective symbol displayed at explanation\n",
        "for matching a single digit we use /d\n",
        "for 2 digits /d/d and so on ..\n",
        "but inorder to simply match all the 10 digits at a time we use /d{10} = excatly 10 of d(0-9)'''"
      ],
      "metadata": {
        "id": "8NShW27jYAgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EKfZdzX4X5iI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat1='hello welcome! whats up ,to contact me msg to 9182017247 or mailto lavanyavaranasi3@gmail.com'\n",
        "chat2=\"hello welcome! whats up ,to contact me msg to 918-201-7247 or mailto lavanyavaranasi3@gmail.com\""
      ],
      "metadata": {
        "id": "BD2ihFkDFD8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matchcheker=re.findall('\\d{10}',chat1)\n",
        "matchcheker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyKCXTSYGazY",
        "outputId": "6f47038c-d6bf-443e-faac-792c3bc33e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['9182017247']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "''' in order to match 091-233-4554 this kind of number we use \" \\(d{3}\\)-\\d{3}-\\d{4} '''"
      ],
      "metadata": {
        "id": "SnbyHYIaYIvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "match2=re.findall('\\d{10}|\\d{3}\\-\\d{3}-\\d{4}',chat2)\n",
        "match2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJR4CgcEKN0J",
        "outputId": "052055de-f4e2-41e6-f60e-f8af69c27bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['918-201-7247']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "f8FpBKCpTPrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** using spacy (oops)**"
      ],
      "metadata": {
        "id": "jApihrlrRfTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\"Dr. Lavanya ! this is me , and you will. whats up?\")\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUTO3rxzR1ev",
        "outputId": "821d6ee3-33d9-4faf-8650-eb6f082f8231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr. Lavanya !\n",
            "this is me , and you will.\n",
            "whats up?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in doc.sents:\n",
        "  for words in sentence:\n",
        "    print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bm3FTZxTIjc",
        "outputId": "8dfc684b-1724-40b0-e47f-0fca6d0f9081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr.\n",
            "lavanya\n",
            "!\n",
            "this\n",
            "is\n",
            "me\n",
            ",\n",
            "and\n",
            "you\n",
            "will\n",
            ".\n",
            "what\n",
            "s\n",
            "up\n",
            "?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*NLTK (string processing)*"
      ],
      "metadata": {
        "id": "OfK7yGXXUElg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJScnd0RUNfL",
        "outputId": "3dbbed33-b5fc-42b9-af1a-5242a0476649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SENTENCE TOKENIZATION**"
      ],
      "metadata": {
        "id": "Qt5InLwrYYZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize( \"Dr. Lavanya ! this is me , and you will. whats up?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjIADPNLUT7G",
        "outputId": "9c888cc1-3435-4324-c4c6-19a3f15c7fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr. Lavanya !', 'this is me , and you will.', 'whats up?']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEED TO VERIFY THIS *In spacy if you choose sentence tokenizer and gave input as Dr. then it considers as word itself but if you consider NLTK it takes it as another sentence *"
      ],
      "metadata": {
        "id": "2Cxej83wXCxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORD TOKENIZER**"
      ],
      "metadata": {
        "id": "cQyXQsLyYk9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(\"Dr. lavanya ! this is me , and you will. whats up?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-KEeO4UXt0f",
        "outputId": "9b2c84d3-f495-4e9d-bb21-0e06c5e27189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello',\n",
              " 'dr.',\n",
              " 'lavanya',\n",
              " '!',\n",
              " 'this',\n",
              " 'is',\n",
              " 'me',\n",
              " ',',\n",
              " 'and',\n",
              " 'you',\n",
              " 'will',\n",
              " '.',\n",
              " 'whats',\n",
              " 'up',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. *: Matches zero or more occurrences.\n",
        "2. +: Matches one or more occurrences.\n",
        "3.  ?: Matches zero or one occurrence.\n",
        "4.  {n}: Matches exactly n occurrences.\n",
        "5.  {n,}: Matches n or more occurrences.\n",
        "6.  {n,m}: Matches at least n and at most m occurrences.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l-z4ugtMBMH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_noise(text):\n",
        "    # Remove noise containing common words\n",
        "    noise_words = ['is', 'a', 'this']\n",
        "    cleaned_text_words = text\n",
        "    for word in noise_words:\n",
        "        pattern = r'\\b{}\\b'.format(word)\n",
        "        cleaned_text_words = re.sub(pattern, '', cleaned_text_words)\n",
        "\n",
        "    # Remove words following hashtags\n",
        "    cleaned_text_hashtags = re.sub(r'#\\w+\\s*', '', text)\n",
        "\n",
        "    # Remove \"@\" symbols\n",
        "    cleaned_text_hashtags = re.sub(r'@\\w+', '', cleaned_text_hashtags)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    cleaned_text_hashtags = re.sub(r'\\s+', ' ', cleaned_text_hashtags)\n",
        "\n",
        "    return cleaned_text_words, cleaned_text_hashtags\n",
        "\n",
        "# Example text with noise containing common words and hashtags\n",
        "text = \"This is a #sample text with @some noise!   12345 #removeme #notthisone\"\n",
        "\n",
        "# Remove noise containing common words and hashtags separately\n",
        "cleaned_text_words, cleaned_text_hashtags = remove_noise(text)\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nCleaned Text (Removing noise containing common words):\")\n",
        "print(cleaned_text_words)\n",
        "print(\"\\nCleaned Text (Removing words following hashtags):\")\n",
        "print(cleaned_text_hashtags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsL-1iRLNv2S",
        "outputId": "1aa562bb-75c8-477a-dcfc-60d096657f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "This is a #sample text with @some noise!   12345 #removeme #notthisone\n",
            "\n",
            "Cleaned Text (Removing noise containing common words):\n",
            "This   #sample text with @some noise!   12345 #removeme #notthisone\n",
            "\n",
            "Cleaned Text (Removing words following hashtags):\n",
            "This is a text with noise! 12345 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def noise_removal(text):\n",
        "  noise_words=['is','a','this']\n",
        "  clean_text=text\n",
        "  for word in noise_words:\n",
        "    position=r'\\b{}\\b'.format(word)\n",
        "    clean_text=re.sub(position,'',clean_text)\n",
        "  hashtag_removal=re.sub(r'#\\w+\\s|@\\w+\\s','',text)\n",
        "  return clean_text,hashtag_removal\n",
        "text=\"this code is very #easy @to  do  \"\n",
        "clean_text,hashtag_removal=noise_removal(text)\n",
        "print(text)\n",
        "print(clean_text)\n",
        "print(hashtag_removal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jODJ109pYEfu",
        "outputId": "d80fc0a6-8dd3-4015-dd15-9ba1bede2233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this code is very #easy @to  do  \n",
            " code  very #easy @to  do  \n",
            "this code is very  do  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def noise_removal(text):\n",
        "  for word in text:\n",
        "        pattern = r'\\b{}\\b'.format(word)\n",
        "  hashremoval=re.sub(r'@\\w+|#\\w+\\s|\\s',' ',text)\n",
        "  return hashremoval\n",
        "text=\"this code is very #easy is @it \"\n",
        "print(noise_removal(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBJYrBHCcuj9",
        "outputId": "b72a268e-90ad-4f93-c6cf-6f9b5123b6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this code is very  is   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**media slangs to standard text**"
      ],
      "metadata": {
        "id": "7Uu13Q-GUD6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slangs_map={\n",
        "    \"lol\":\"laugh out loud\",\n",
        "    \"omg\":\"oh my god\",\n",
        "    \"hlo\":\"hello\",\n",
        "    \"tq\":\"thank you\"\n",
        "}\n",
        "def standard_text(text,slangs_map):\n",
        "  for slang,standard in slangs_map.items():\n",
        "    text=text.replace(slang,standard)\n",
        "  return text\n",
        "text=\"hlo the movie omg that you suggested was lol tq for it\"\n",
        "standardized_text=standard_text(text,slangs_map)\n",
        "print(text)\n",
        "print(standardized_text)"
      ],
      "metadata": {
        "id": "AEycaun3Sp6-",
        "outputId": "505babc2-9bea-4d0a-b307-8a11061273c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hlo the movie omg that you suggested was lol tq for it\n",
            "hello the movie oh my god that you suggested was laugh out loud thank you for it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text=\"hi my name is lavanya , i love to improve myself\"\n",
        "tokens=word_tokenize(text)\n",
        "print(tokens)\n",
        "pos_tagger=pos_tag(tokens)\n",
        "print(pos_tagger)\n"
      ],
      "metadata": {
        "id": "4GL9wpUpUMkY",
        "outputId": "112a094b-59af-42ac-9da0-22d11d26146b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi', 'my', 'name', 'is', 'lavanya', ',', 'i', 'love', 'to', 'improve', 'myself']\n",
            "[('hi', 'NN'), ('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('lavanya', 'JJ'), (',', ','), ('i', 'JJ'), ('love', 'VBP'), ('to', 'TO'), ('improve', 'VB'), ('myself', 'PRP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def pos_tagger(text):\n",
        "  tokens=word_tokenize(text)\n",
        "  pos_framer=pos_tag(tokens)\n",
        "  return pos_framer\n",
        "text=\"hi my name is lavanya , i love to improve myself\"\n",
        "pos_converter=pos_tagger(text)\n",
        "print(text)\n",
        "print(pos_converter)"
      ],
      "metadata": {
        "id": "-1yW1NQeWDHp",
        "outputId": "870039a9-0e23-430f-e7e0-ee1771b1a884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi my name is lavanya , i love to improve myself\n",
            "[('hi', 'NN'), ('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('lavanya', 'JJ'), (',', ','), ('i', 'JJ'), ('love', 'VBP'), ('to', 'TO'), ('improve', 'VB'), ('myself', 'PRP')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Sample documents\n",
        "documents = [\n",
        "    \"data science is one of the most important fields of science\",\n",
        "    \"this is one of the best data science courses\",\n",
        "    \"data scientists analyze data\"\n",
        "]\n",
        "\n",
        "# Step 2: Tokenization\n",
        "def tokenize(document):\n",
        "    return document.lower().split()\n",
        "\n",
        "# Step 3: Calculate Term Frequency (TF)\n",
        "def calculate_tf(document):\n",
        "    tf_counts = Counter(tokenize(document))\n",
        "    total_terms = len(tokenize(document))\n",
        "    tf = {term: count / total_terms for term, count in tf_counts.items()}\n",
        "    return tf\n",
        "\n",
        "# Step 4: Calculate Inverse Document Frequency (IDF)\n",
        "def calculate_idf(documents):\n",
        "    total_documents = len(documents)\n",
        "    idf = {}\n",
        "    all_terms = set([term for doc in documents for term in tokenize(doc)])\n",
        "\n",
        "    for term in all_terms:\n",
        "        documents_with_term = sum([1 for doc in documents if term in tokenize(doc)])\n",
        "        idf[term] = math.log(total_documents / (1 + documents_with_term))\n",
        "\n",
        "    return idf\n",
        "\n",
        "# Step 5: Calculate TF-IDF\n",
        "def calculate_tfidf(tf, idf):\n",
        "    tfidf = {term: tf_val * idf.get(term, 0) for term, tf_val in tf.items()}\n",
        "    return tfidf\n",
        "\n",
        "# Calculate TF-IDF for each document\n",
        "tf_idf_scores = []\n",
        "for document in documents:\n",
        "    tf = calculate_tf(document)\n",
        "    idf = calculate_idf(documents)\n",
        "    tfidf = calculate_tfidf(tf, idf)\n",
        "    tf_idf_scores.append(tfidf)\n",
        "\n",
        "# Display TF-IDF scores for each document\n",
        "for i, tfidf_scores in enumerate(tf_idf_scores):\n",
        "    print(f\"TF-IDF scores for document {i+1}:\")\n",
        "    for term, score in tfidf_scores.items():\n",
        "        print(f\"{term}: {score:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "hr-YrMpXDyL8",
        "outputId": "b5e91ae2-7daa-47c9-eb08-0cb721e6b4e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF scores for document 1:\n",
            "data: -0.0262\n",
            "science: 0.0000\n",
            "is: 0.0000\n",
            "one: 0.0000\n",
            "of: 0.0000\n",
            "the: 0.0000\n",
            "most: 0.0369\n",
            "important: 0.0369\n",
            "fields: 0.0369\n",
            "\n",
            "TF-IDF scores for document 2:\n",
            "this: 0.0451\n",
            "is: 0.0000\n",
            "one: 0.0000\n",
            "of: 0.0000\n",
            "the: 0.0000\n",
            "best: 0.0451\n",
            "data: -0.0320\n",
            "science: 0.0000\n",
            "courses: 0.0451\n",
            "\n",
            "TF-IDF scores for document 3:\n",
            "data: -0.1438\n",
            "scientists: 0.1014\n",
            "analyze: 0.1014\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"data science is one of the most important fields of science\",\n",
        "    \"this is one of the best data science courses\",\n",
        "    \"data scientists analyze data\"\n",
        "]\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the documents and transform the documents into TF-IDF vectors\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names (terms)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Display TF-IDF scores for each document\n",
        "for i, document in enumerate(documents):\n",
        "    print(f\"TF-IDF scores for document {i+1}:\")\n",
        "    for j, term in enumerate(feature_names):\n",
        "        print(f\"{term}: {tfidf_vectors[i, j]:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "YJiB-qRXMDJz",
        "outputId": "e68c7ca2-7af6-4ac6-812a-666f728b73fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF scores for document 1:\n",
            "analyze: 0.0000\n",
            "best: 0.0000\n",
            "courses: 0.0000\n",
            "data: 0.1895\n",
            "fields: 0.3209\n",
            "important: 0.3209\n",
            "is: 0.2440\n",
            "most: 0.3209\n",
            "of: 0.4881\n",
            "one: 0.2440\n",
            "science: 0.4881\n",
            "scientists: 0.0000\n",
            "the: 0.2440\n",
            "this: 0.0000\n",
            "\n",
            "TF-IDF scores for document 2:\n",
            "analyze: 0.0000\n",
            "best: 0.4003\n",
            "courses: 0.4003\n",
            "data: 0.2364\n",
            "fields: 0.0000\n",
            "important: 0.0000\n",
            "is: 0.3044\n",
            "most: 0.0000\n",
            "of: 0.3044\n",
            "one: 0.3044\n",
            "science: 0.3044\n",
            "scientists: 0.0000\n",
            "the: 0.3044\n",
            "this: 0.4003\n",
            "\n",
            "TF-IDF scores for document 3:\n",
            "analyze: 0.5427\n",
            "best: 0.0000\n",
            "courses: 0.0000\n",
            "data: 0.6411\n",
            "fields: 0.0000\n",
            "important: 0.0000\n",
            "is: 0.0000\n",
            "most: 0.0000\n",
            "of: 0.0000\n",
            "one: 0.0000\n",
            "science: 0.0000\n",
            "scientists: 0.5427\n",
            "the: 0.0000\n",
            "this: 0.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Example corpus (list of documents)\n",
        "corpus = [\n",
        " \"data science is one of the most important fields of science\",\n",
        "    \"this is one of the best data science courses\",\n",
        "    \"data scientists analyze data\"\n",
        "]\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "# Fit the vectorizer on the corpus and transform the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "# Convert the TF-IDF matrix to a dense array for easier manipulation\n",
        "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
        "# Get feature names (terms)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "# Print the TF-IDF matrix\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix_dense)\n",
        "# Print feature names (terms)\n",
        "print(\"\\nFeature Names:\")\n",
        "print(feature_names)"
      ],
      "metadata": {
        "id": "XkeQO8UENzoV",
        "outputId": "f1fb7a75-5ed9-46b8-9459-efe7bf0c3769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.18952581 0.32089509 0.32089509\n",
            "  0.24404899 0.32089509 0.48809797 0.24404899 0.48809797 0.\n",
            "  0.24404899 0.        ]\n",
            " [0.         0.40029393 0.40029393 0.23642005 0.         0.\n",
            "  0.30443385 0.         0.30443385 0.30443385 0.30443385 0.\n",
            "  0.30443385 0.40029393]\n",
            " [0.54270061 0.         0.         0.64105545 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.54270061\n",
            "  0.         0.        ]]\n",
            "\n",
            "Feature Names:\n",
            "['analyze' 'best' 'courses' 'data' 'fields' 'important' 'is' 'most' 'of'\n",
            " 'one' 'science' 'scientists' 'the' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "corpus=[\"data science is one of the most important fields of science\",\n",
        "    \"this is one of the best data science courses\",\n",
        "    \"data scientists analyze data\"]\n",
        "words_set=set()\n",
        "for doc in corpus:\n",
        "  words=doc.split(' ')\n",
        "  words_set=words_set.union(set(words))\n",
        "n_docs=len(corpus)\n",
        "n_words_set=len(words_set)\n",
        "df_tf=pd.DataFrame(np.zeros((n_docs,n_words_set)),columns=list(words_set))\n",
        "#tf\n",
        "for i in range(n_docs):\n",
        "  words=corpus[i].split(' ')\n",
        "  for w in words:\n",
        "    df_tf[w][i]=df_tf[w][i]+(1/len(words))\n",
        "#idf\n",
        "idf={}\n",
        "for w in words_set:\n",
        "  k=0\n",
        "  for i in range(n_docs):\n",
        "    if w in corpus[i].split(' '):\n",
        "      k+=1\n",
        "  idf[w]=np.log10(n_docs/k)\n",
        "#df_tf_idf\n",
        "df_tf_idf=df_tf.copy()\n",
        "for w in words_set:\n",
        "  for i in range(n_docs):\n",
        "    df_tf_idf[w][i]=df_tf[w][i]*idf[w]\n",
        "print(df_tf_idf.sort_index(axis=1))\n"
      ],
      "metadata": {
        "id": "nydIR9BjdWM6",
        "outputId": "122debf7-194e-4cc7-892d-1af78cd31047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   analyze      best   courses  data    fields  important        is      most  \\\n",
            "0  0.00000  0.000000  0.000000   0.0  0.043375   0.043375  0.016008  0.043375   \n",
            "1  0.00000  0.053013  0.053013   0.0  0.000000   0.000000  0.019566  0.000000   \n",
            "2  0.11928  0.000000  0.000000   0.0  0.000000   0.000000  0.000000  0.000000   \n",
            "\n",
            "         of       one   science  scientists       the      this  \n",
            "0  0.032017  0.016008  0.032017     0.00000  0.016008  0.000000  \n",
            "1  0.019566  0.019566  0.019566     0.00000  0.019566  0.053013  \n",
            "2  0.000000  0.000000  0.000000     0.11928  0.000000  0.000000  \n"
          ]
        }
      ]
    }
  ]
}